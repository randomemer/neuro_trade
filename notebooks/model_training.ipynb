{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import peewee\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n",
      "Path added\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "root_dir = os.path.dirname(\"../\")\n",
    "print(root_dir)\n",
    "\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "    print(\"Path added\")\n",
    "else:\n",
    "    print(\"Path exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.data import create_dataloaders\n",
    "from src.model.main import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = peewee.SqliteDatabase(\"../data/dataset.sqlite3\")\n",
    "conn = db.connection()\n",
    "tables = db.get_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 338142 entries, 0 to 338141\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   symbol     338142 non-null  object \n",
      " 1   timestamp  338142 non-null  object \n",
      " 2   open       338142 non-null  float64\n",
      " 3   high       338142 non-null  float64\n",
      " 4   low        338142 non-null  float64\n",
      " 5   close      338142 non-null  float64\n",
      " 6   volume     338142 non-null  float64\n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 18.1+ MB\n"
     ]
    }
   ],
   "source": [
    "stock = \"AAPL\"\n",
    "stock_df = pd.read_sql(f\"SELECT * FROM {tables[0]} WHERE symbol = '{stock}'\", conn)\n",
    "stock_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5  # OHLCV\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "output_size = 5  # Predicting OHLCV\n",
    "\n",
    "seq_length = 10\n",
    "batch_size = 32\n",
    "test_size = 0.2\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cols ['open', 'high', 'low', 'close', 'volume']\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, test_dataloader = create_dataloaders(stock_df, seq_length, batch_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size, hidden_size, num_layers, output_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2.0033e-01, 1.9989e-01, 2.0199e-01, 2.0075e-01, 2.1918e-06],\n",
      "         [2.0026e-01, 1.9989e-01, 2.0173e-01, 2.0075e-01, 2.0905e-05],\n",
      "         [2.0031e-01, 1.9987e-01, 2.0197e-01, 2.0073e-01, 1.5370e-06],\n",
      "         ...,\n",
      "         [2.0009e-01, 1.9977e-01, 2.0175e-01, 2.0061e-01, 8.7672e-06],\n",
      "         [2.0019e-01, 1.9980e-01, 2.0178e-01, 2.0054e-01, 6.0929e-06],\n",
      "         [2.0024e-01, 1.9980e-01, 2.0190e-01, 2.0066e-01, 3.0051e-06]],\n",
      "\n",
      "        [[1.1992e-01, 1.1966e-01, 1.2058e-01, 1.1968e-01, 1.0792e-01],\n",
      "         [1.1920e-01, 1.1922e-01, 1.2003e-01, 1.1893e-01, 1.9579e-04],\n",
      "         [1.1849e-01, 1.1847e-01, 1.2004e-01, 1.1910e-01, 9.6542e-05],\n",
      "         ...,\n",
      "         [1.1699e-01, 1.1741e-01, 1.1826e-01, 1.1813e-01, 5.0927e-04],\n",
      "         [1.1739e-01, 1.1922e-01, 1.1881e-01, 1.1764e-01, 1.4236e-04],\n",
      "         [1.1710e-01, 1.1704e-01, 1.1850e-01, 1.1757e-01, 1.0614e-04]],\n",
      "\n",
      "        [[3.0450e-01, 3.0449e-01, 3.0579e-01, 3.0476e-01, 3.9283e-03],\n",
      "         [3.0446e-01, 3.0431e-01, 3.0598e-01, 3.0489e-01, 2.4452e-03],\n",
      "         [3.0454e-01, 3.0457e-01, 3.0603e-01, 3.0543e-01, 2.3401e-03],\n",
      "         ...,\n",
      "         [3.0660e-01, 3.0635e-01, 3.0808e-01, 3.0726e-01, 2.8622e-03],\n",
      "         [3.0688e-01, 3.0696e-01, 3.0848e-01, 3.0756e-01, 2.4936e-03],\n",
      "         [3.0714e-01, 3.0658e-01, 3.0756e-01, 3.0662e-01, 2.7256e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[9.9165e-03, 9.8996e-03, 1.0933e-02, 1.0031e-02, 2.9997e-03],\n",
      "         [9.4746e-03, 9.8058e-03, 1.0815e-02, 1.0125e-02, 3.2392e-03],\n",
      "         [9.5451e-03, 1.0134e-02, 1.0957e-02, 1.0594e-02, 4.7165e-03],\n",
      "         ...,\n",
      "         [1.0109e-02, 1.0416e-02, 1.1286e-02, 1.0324e-02, 6.0047e-03],\n",
      "         [9.7567e-03, 1.0017e-02, 1.0651e-02, 9.8896e-03, 9.1929e-03],\n",
      "         [9.2865e-03, 9.5243e-03, 1.0839e-02, 9.8661e-03, 3.4984e-02]],\n",
      "\n",
      "        [[1.9864e-01, 1.9830e-01, 2.0011e-01, 1.9904e-01, 1.1614e-03],\n",
      "         [1.9864e-01, 1.9832e-01, 2.0019e-01, 1.9895e-01, 1.3197e-03],\n",
      "         [1.9854e-01, 1.9818e-01, 2.0002e-01, 1.9880e-01, 1.8961e-03],\n",
      "         ...,\n",
      "         [1.9901e-01, 1.9907e-01, 2.0060e-01, 1.9955e-01, 3.8051e-03],\n",
      "         [1.9911e-01, 1.9874e-01, 2.0046e-01, 1.9941e-01, 8.1283e-03],\n",
      "         [1.9913e-01, 1.9870e-01, 2.0032e-01, 1.9932e-01, 2.9216e-02]],\n",
      "\n",
      "        [[1.9518e-01, 1.9518e-01, 1.9679e-01, 1.9589e-01, 1.2864e-03],\n",
      "         [1.9546e-01, 1.9517e-01, 1.9686e-01, 1.9565e-01, 1.2875e-03],\n",
      "         [1.9522e-01, 1.9482e-01, 1.9628e-01, 1.9506e-01, 1.8432e-03],\n",
      "         ...,\n",
      "         [1.9429e-01, 1.9445e-01, 1.9595e-01, 1.9492e-01, 2.3488e-03],\n",
      "         [1.9448e-01, 1.9417e-01, 1.9571e-01, 1.9467e-01, 2.1005e-03],\n",
      "         [1.9423e-01, 1.9382e-01, 1.9552e-01, 1.9453e-01, 2.2183e-03]]],\n",
      "       dtype=torch.float64) tensor([[2.0012e-01, 1.9968e-01, 2.0178e-01, 2.0054e-01, 2.0677e-06],\n",
      "        [1.1715e-01, 1.1734e-01, 1.1852e-01, 1.1804e-01, 1.2609e-04],\n",
      "        [3.0631e-01, 3.0605e-01, 3.0753e-01, 3.0688e-01, 2.1946e-03],\n",
      "        [3.2679e-02, 3.3053e-02, 3.3975e-02, 3.3686e-02, 9.3197e-04],\n",
      "        [8.1509e-02, 8.1472e-02, 8.3138e-02, 8.2147e-02, 3.2229e-05],\n",
      "        [1.9877e-01, 1.9837e-01, 2.0009e-01, 1.9892e-01, 8.7498e-04],\n",
      "        [1.4941e-01, 1.4908e-01, 1.5080e-01, 1.4961e-01, 3.2505e-05],\n",
      "        [4.1117e-01, 4.1101e-01, 4.1277e-01, 4.1202e-01, 5.4909e-04],\n",
      "        [2.4084e-01, 2.4151e-01, 2.4229e-01, 2.4238e-01, 1.8942e-03],\n",
      "        [2.5908e-01, 2.5856e-01, 2.6023e-01, 2.5894e-01, 8.6603e-05],\n",
      "        [2.0310e-01, 2.0278e-01, 2.0476e-01, 2.0364e-01, 7.1399e-05],\n",
      "        [1.6111e-01, 1.6097e-01, 1.6265e-01, 1.6157e-01, 3.0637e-05],\n",
      "        [1.8075e-01, 1.8035e-01, 1.8210e-01, 1.8097e-01, 1.9002e-05],\n",
      "        [1.8016e-01, 1.8012e-01, 1.8182e-01, 1.8060e-01, 1.2592e-05],\n",
      "        [1.7414e-01, 1.7392e-01, 1.7406e-01, 1.7306e-01, 4.5835e-03],\n",
      "        [4.8066e-02, 4.7973e-02, 4.9425e-02, 4.8438e-02, 2.8765e-03],\n",
      "        [1.0349e-01, 1.0334e-01, 1.0512e-01, 1.0404e-01, 5.5746e-05],\n",
      "        [3.0775e-01, 3.0731e-01, 3.0906e-01, 3.0832e-01, 6.0970e-05],\n",
      "        [2.7577e-01, 2.7541e-01, 2.7744e-01, 2.7613e-01, 4.0596e-05],\n",
      "        [2.9345e-01, 2.9708e-01, 2.9096e-01, 2.9448e-01, 2.8286e-02],\n",
      "        [4.8384e-02, 4.8513e-02, 4.9610e-02, 4.8837e-02, 8.1140e-03],\n",
      "        [2.0642e-01, 2.0620e-01, 2.0792e-01, 2.0686e-01, 5.9688e-05],\n",
      "        [2.0444e-01, 2.0400e-01, 2.0596e-01, 2.0472e-01, 4.8178e-05],\n",
      "        [4.3376e-02, 4.3751e-02, 4.4719e-02, 4.3904e-02, 1.2025e-02],\n",
      "        [1.9824e-01, 1.9935e-01, 1.9990e-01, 2.0009e-01, 1.2799e-03],\n",
      "        [1.8627e-01, 1.8678e-01, 1.8793e-01, 1.8749e-01, 1.7186e-03],\n",
      "        [6.1607e-01, 6.1538e-01, 6.1636e-01, 6.1553e-01, 3.0566e-03],\n",
      "        [9.1407e-02, 9.1348e-02, 9.2754e-02, 9.1966e-02, 3.3182e-04],\n",
      "        [8.6301e-01, 8.6112e-01, 8.6406e-01, 8.6223e-01, 2.4420e-05],\n",
      "        [9.3805e-03, 9.5008e-03, 1.1004e-02, 1.0101e-02, 3.4823e-04],\n",
      "        [1.9880e-01, 1.9848e-01, 2.0034e-01, 1.9934e-01, 7.7533e-05],\n",
      "        [1.9412e-01, 1.9417e-01, 1.9571e-01, 1.9495e-01, 1.9558e-03]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not at the same device, found input tensor at cuda:0 and parameter tensor at cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\Codes\\python\\neuro_trade\\notebooks\\model_training.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Codes/python/neuro_trade/notebooks/model_training.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Codes/python/neuro_trade/notebooks/model_training.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/Codes/python/neuro_trade/notebooks/model_training.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Codes/python/neuro_trade/notebooks/model_training.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Codes/python/neuro_trade/notebooks/model_training.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\Documents\\Codes\\python\\neuro_trade\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Documents\\Codes\\python\\neuro_trade\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Documents\\Codes\\python\\neuro_trade\\notebooks\\..\\src\\model\\main.py:28\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m h0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     26\u001b[0m c0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> 28\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (h0, c0))\n\u001b[0;32m     29\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n\u001b[0;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\Documents\\Codes\\python\\neuro_trade\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Documents\\Codes\\python\\neuro_trade\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Documents\\Codes\\python\\neuro_trade\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    876\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    878\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    880\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    881\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    882\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    883\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cuda:0 and parameter tensor at cpu"
     ]
    }
   ],
   "source": [
    "device = model.device\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, (inputs, targets) in enumerate(train_dataloader):\n",
    "        inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
